{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm  \n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import time\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define global constants for dataset directories\n",
    "DATA_DIR = '/kaggle/input/byu-locating-bacterial-flagellar-motors-2025'\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, 'train_labels.csv')\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "TEST_DIR = os.path.join(DATA_DIR, 'test')\n",
    "OUTPUT_DIR = './'\n",
    "MODEL_DIR = './models'\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Set device: Use GPU if available; otherwise, fall back to CPU\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motors in the dataset: 831\n",
      "Found 362 unique tomograms with motors\n",
      "Split: 289 tomograms for training, 73 tomograms for validation\n",
      "Will process approximately 3267 slices for training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95bb0bd2614449197d6fc33471e997b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing training motors:   0%|          | 0/363 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will process approximately 792 slices for validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd41d2b38883480da9e69682a9df7ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing validation motors:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Summary:\n",
      "- Train set: 289 tomograms, 363 motors, 3262 slices\n",
      "- Validation set: 73 tomograms, 88 motors, 792 slices\n",
      "- Total: 362 tomograms, 451 motors, 4054 slices\n",
      "\n",
      "Preprocessing Complete:\n",
      "- Training data: 289 tomograms, 363 motors, 3262 slices\n",
      "- Validation data: 73 tomograms, 88 motors, 792 slices\n",
      "- Dataset directory: kaggle/working/yolo_dataset\n",
      "- YAML configuration: kaggle/working/yolo_dataset/dataset.yaml\n",
      "\n",
      "Ready for YOLO training!\n"
     ]
    }
   ],
   "source": [
    "# Define YOLO dataset structure and parameters\n",
    "data_path = \"kaggle/input/byu-locating-bacterial-flagellar-motors-2025/\"\n",
    "train_dir = os.path.join(data_path, \"train\")\n",
    "\n",
    "# Output directories for YOLO dataset (adjust as needed)\n",
    "yolo_dataset_dir = \"kaggle/working/yolo_dataset\"\n",
    "yolo_images_train = os.path.join(yolo_dataset_dir, \"images\", \"train\")\n",
    "yolo_images_val = os.path.join(yolo_dataset_dir, \"images\", \"val\")\n",
    "yolo_labels_train = os.path.join(yolo_dataset_dir, \"labels\", \"train\")\n",
    "yolo_labels_val = os.path.join(yolo_dataset_dir, \"labels\", \"val\")\n",
    "\n",
    "# Create necessary directories\n",
    "for dir_path in [yolo_images_train, yolo_images_val, yolo_labels_train, yolo_labels_val]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Define constants for processing\n",
    "TRUST = 4       # Number of slices above and below center slice (total slices = 2*TRUST + 1)\n",
    "BOX_SIZE = 24   # Bounding box size (in pixels)\n",
    "TRAIN_SPLIT = 0.8  # 80% training, 20% validation\n",
    "\n",
    "# Define a helper function for image normalization using percentile-based contrast enhancement.\n",
    "def normalize_slice(slice_data):\n",
    "    \"\"\"\n",
    "    Normalize slice data using the 2nd and 98th percentiles.\n",
    "    \n",
    "    Args:\n",
    "        slice_data (numpy.array): Input image slice.\n",
    "    \n",
    "    Returns:\n",
    "        np.uint8: Normalized image in the range [0, 255].\n",
    "    \"\"\"\n",
    "    p2 = np.percentile(slice_data, 2)\n",
    "    p98 = np.percentile(slice_data, 98)\n",
    "    clipped_data = np.clip(slice_data, p2, p98)\n",
    "    normalized = 255 * (clipped_data - p2) / (p98 - p2)\n",
    "    return np.uint8(normalized)\n",
    "\n",
    "# Define the preprocessing function to extract slices, normalize, and generate YOLO annotations.\n",
    "def prepare_yolo_dataset(trust=TRUST, train_split=TRAIN_SPLIT):\n",
    "    \"\"\"\n",
    "    Extract slices containing motors and save images with corresponding YOLO annotations.\n",
    "    \n",
    "    Steps:\n",
    "    - Load the motor labels.\n",
    "    - Perform a train/validation split by tomogram.\n",
    "    - For each motor, extract slices in a range (Â± trust parameter).\n",
    "    - Normalize each slice and save it.\n",
    "    - Generate YOLO format bounding box annotations with a fixed box size.\n",
    "    - Create a YAML configuration file for YOLO training.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A summary containing dataset statistics and file paths.\n",
    "    \"\"\"\n",
    "    # Load the labels CSV\n",
    "    labels_df = pd.read_csv(os.path.join(data_path, \"train_labels.csv\"))\n",
    "    \n",
    "    total_motors = labels_df['Number of motors'].sum()\n",
    "    print(f\"Total number of motors in the dataset: {total_motors}\")\n",
    "    \n",
    "    # Consider only tomograms with at least one motor\n",
    "    tomo_df = labels_df[labels_df['Number of motors'] > 0].copy()\n",
    "    unique_tomos = tomo_df['tomo_id'].unique()\n",
    "    print(f\"Found {len(unique_tomos)} unique tomograms with motors\")\n",
    "    \n",
    "    # Shuffle and split tomograms into train and validation sets\n",
    "    np.random.shuffle(unique_tomos)\n",
    "    split_idx = int(len(unique_tomos) * train_split)\n",
    "    train_tomos = unique_tomos[:split_idx]\n",
    "    val_tomos = unique_tomos[split_idx:]\n",
    "    print(f\"Split: {len(train_tomos)} tomograms for training, {len(val_tomos)} tomograms for validation\")\n",
    "    \n",
    "    # Helper function to process a list of tomograms\n",
    "    def process_tomogram_set(tomogram_ids, images_dir, labels_dir, set_name):\n",
    "        motor_counts = []\n",
    "        for tomo_id in tomogram_ids:\n",
    "            # Get motor annotations for the current tomogram\n",
    "            tomo_motors = labels_df[labels_df['tomo_id'] == tomo_id]\n",
    "            for _, motor in tomo_motors.iterrows():\n",
    "                if pd.isna(motor['Motor axis 0']):\n",
    "                    continue\n",
    "                motor_counts.append(\n",
    "                    (tomo_id, \n",
    "                     int(motor['Motor axis 0']), \n",
    "                     int(motor['Motor axis 1']), \n",
    "                     int(motor['Motor axis 2']),\n",
    "                     int(motor['Array shape (axis 0)']))\n",
    "                )\n",
    "        \n",
    "        print(f\"Will process approximately {len(motor_counts) * (2 * trust + 1)} slices for {set_name}\")\n",
    "        processed_slices = 0\n",
    "        \n",
    "        # Loop over each motor annotation\n",
    "        for tomo_id, z_center, y_center, x_center, z_max in tqdm(motor_counts, desc=f\"Processing {set_name} motors\"):\n",
    "            z_min = max(0, z_center - trust)\n",
    "            z_max_bound = min(z_max - 1, z_center + trust)\n",
    "            for z in range(z_min, z_max_bound + 1):\n",
    "                # Create the slice filename and source path\n",
    "                slice_filename = f\"slice_{z:04d}.jpg\"\n",
    "                src_path = os.path.join(train_dir, tomo_id, slice_filename)\n",
    "                if not os.path.exists(src_path):\n",
    "                    print(f\"Warning: {src_path} does not exist, skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                # Load, normalize, and save the image slice\n",
    "                img = Image.open(src_path)\n",
    "                img_array = np.array(img)\n",
    "                normalized_img = normalize_slice(img_array)\n",
    "                dest_filename = f\"{tomo_id}_z{z:04d}_y{y_center:04d}_x{x_center:04d}.jpg\"\n",
    "                dest_path = os.path.join(images_dir, dest_filename)\n",
    "                Image.fromarray(normalized_img).save(dest_path)\n",
    "                \n",
    "                # Prepare YOLO bounding box annotation (normalized values)\n",
    "                img_width, img_height = img.size\n",
    "                x_center_norm = x_center / img_width\n",
    "                y_center_norm = y_center / img_height\n",
    "                box_width_norm = BOX_SIZE / img_width\n",
    "                box_height_norm = BOX_SIZE / img_height\n",
    "                label_path = os.path.join(labels_dir, dest_filename.replace('.jpg', '.txt'))\n",
    "                with open(label_path, 'w') as f:\n",
    "                    f.write(f\"0 {x_center_norm} {y_center_norm} {box_width_norm} {box_height_norm}\\n\")\n",
    "                \n",
    "                processed_slices += 1\n",
    "        \n",
    "        return processed_slices, len(motor_counts)\n",
    "    \n",
    "    # Process training tomograms\n",
    "    train_slices, train_motors = process_tomogram_set(train_tomos, yolo_images_train, yolo_labels_train, \"training\")\n",
    "    # Process validation tomograms\n",
    "    val_slices, val_motors = process_tomogram_set(val_tomos, yolo_images_val, yolo_labels_val, \"validation\")\n",
    "    \n",
    "    # Generate YAML configuration for YOLO training\n",
    "    yaml_content = {\n",
    "        'path': yolo_dataset_dir,\n",
    "        'train': 'images/train',\n",
    "        'val': 'images/val',\n",
    "        'names': {0: 'motor'}\n",
    "    }\n",
    "    with open(os.path.join(yolo_dataset_dir, 'dataset.yaml'), 'w') as f:\n",
    "        yaml.dump(yaml_content, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"\\nProcessing Summary:\")\n",
    "    print(f\"- Train set: {len(train_tomos)} tomograms, {train_motors} motors, {train_slices} slices\")\n",
    "    print(f\"- Validation set: {len(val_tomos)} tomograms, {val_motors} motors, {val_slices} slices\")\n",
    "    print(f\"- Total: {len(train_tomos) + len(val_tomos)} tomograms, {train_motors + val_motors} motors, {train_slices + val_slices} slices\")\n",
    "    \n",
    "    return {\n",
    "        \"dataset_dir\": yolo_dataset_dir,\n",
    "        \"yaml_path\": os.path.join(yolo_dataset_dir, 'dataset.yaml'),\n",
    "        \"train_tomograms\": len(train_tomos),\n",
    "        \"val_tomograms\": len(val_tomos),\n",
    "        \"train_motors\": train_motors,\n",
    "        \"val_motors\": val_motors,\n",
    "        \"train_slices\": train_slices,\n",
    "        \"val_slices\": val_slices\n",
    "    }\n",
    "\n",
    "# Run the preprocessing\n",
    "summary = prepare_yolo_dataset(TRUST)\n",
    "print(f\"\\nPreprocessing Complete:\")\n",
    "print(f\"- Training data: {summary['train_tomograms']} tomograms, {summary['train_motors']} motors, {summary['train_slices']} slices\")\n",
    "print(f\"- Validation data: {summary['val_tomograms']} tomograms, {summary['val_motors']} motors, {summary['val_slices']} slices\")\n",
    "print(f\"- Dataset directory: {summary['dataset_dir']}\")\n",
    "print(f\"- YAML configuration: {summary['yaml_path']}\")\n",
    "print(\"\\nReady for YOLO training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcarlofinnegan\u001b[0m (\u001b[33mtraintest\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m This integration is tested and supported for ultralytics v8.0.238 and below.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m             Please report any issues to https://github.com/wandb/wandb/issues with the tag `yolov8`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/carlo/Documents/Github/W&B/wandb/run-20250313_000015-91tnlyeg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/traintest/BYU_Comp/runs/91tnlyeg' target=\"_blank\">good-universe-2</a></strong> to <a href='https://wandb.ai/traintest/BYU_Comp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/traintest/BYU_Comp' target=\"_blank\">https://wandb.ai/traintest/BYU_Comp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/traintest/BYU_Comp/runs/91tnlyeg' target=\"_blank\">https://wandb.ai/traintest/BYU_Comp/runs/91tnlyeg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting YOLO training process...\n",
      "Directory status:\n",
      "- Train images dir exists: True\n",
      "- Val images dir exists: True\n",
      "- Train labels dir exists: True\n",
      "- Val labels dir exists: True\n",
      "Found original dataset.yaml at kaggle/working/yolo_dataset/dataset.yaml\n",
      "Fixing YAML paths in kaggle/working/yolo_dataset/dataset.yaml\n",
      "Created fixed YAML at kaggle/working/fixed_dataset.yaml with path: kaggle/working/yolo_dataset\n",
      "Using YAML file: kaggle/working/fixed_dataset.yaml\n",
      "YAML file contents:\n",
      "names:\n",
      "  0: motor\n",
      "path: kaggle/working/yolo_dataset\n",
      "train: images/train\n",
      "val: images/val\n",
      "\n",
      "\n",
      "Starting YOLO training...\n",
      "Loading pre-trained weights from: yolov8n.pt\n",
      "New https://pypi.org/project/ultralytics/8.3.89 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.88 ðŸš€ Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3070, 7971MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=kaggle/working/fixed_dataset.yaml, epochs=30, time=None, patience=5, batch=2, imgsz=640, save=True, save_period=5, cache=False, device=None, workers=8, project=yolo_weights_dir, name=motor_detector, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=yolo_weights_dir/motor_detector\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/carlo/Documents/Github/W&B/kaggle/working/yolo_dataset/labels/train.cache... 3262 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3262/3262 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/carlo/Documents/Github/W&B/kaggle/working/yolo_dataset/labels/val.cache... 792 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to yolo_weights_dir/motor_detector/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1myolo_weights_dir/motor_detector\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/30     0.326G      3.016      8.243      1.106          0        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1631/1631 [00:44<00:00, 36.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:03<00:00, 59.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792       0.23      0.325      0.165     0.0441\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/30     0.344G      2.708      3.121      1.046          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1631/1631 [00:41<00:00, 38.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:03<00:00, 63.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.389      0.442       0.36      0.108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/30     0.346G      2.499      2.193     0.9886          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1631/1631 [00:41<00:00, 39.32it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:03<00:00, 63.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792       0.39      0.487      0.327       0.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/30     0.346G      2.387      1.932     0.9669          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1631/1631 [00:42<00:00, 38.54it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:03<00:00, 61.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.359      0.462      0.314     0.0877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/30     0.361G      2.239      1.777     0.9466          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1631/1631 [00:43<00:00, 37.88it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:03<00:00, 59.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.538      0.497       0.45      0.111\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "       6/30     0.379G      2.206      1.671     0.9419          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1631/1631 [00:42<00:00, 38.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:03<00:00, 62.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.654      0.606      0.627      0.194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/30     0.379G      2.121      1.556     0.9181          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1631/1631 [00:42<00:00, 38.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:03<00:00, 64.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.696      0.587      0.622      0.194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/30     0.379G      2.046       1.52     0.9072          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1631/1631 [00:40<00:00, 39.84it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:03<00:00, 64.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.627      0.572      0.558      0.188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/30     0.379G      1.993      1.443     0.9037          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1631/1631 [00:40<00:00, 39.82it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:03<00:00, 61.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.741      0.593      0.663      0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/30     0.379G      1.976      1.405     0.8953          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1631/1631 [00:39<00:00, 40.84it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:03<00:00, 65.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.734      0.717      0.718      0.262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/30     0.379G      1.885      1.305     0.8784          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1631/1631 [00:40<00:00, 40.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:03<00:00, 65.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.771      0.649      0.688      0.208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/30     0.379G       1.87      1.308     0.8796          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1631/1631 [00:40<00:00, 40.28it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:03<00:00, 62.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.794      0.678      0.727       0.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/30     0.379G      1.812      1.228     0.8624          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1631/1631 [00:40<00:00, 40.45it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:03<00:00, 62.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.791       0.71      0.752      0.261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/30     0.379G      1.795      1.231     0.8686          2        640:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1246/1631 [00:32<00:09, 40.33it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from ultralytics import YOLO\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import json\n",
    "import wandb\n",
    "from wandb.integration.ultralytics import add_wandb_callback\n",
    "\n",
    "\n",
    "\n",
    "wandb.init(project=\"BYU_Comp\", job_type=\"training\")\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define paths for Kaggle environment\n",
    "yolo_dataset_dir = \"kaggle/working/yolo_dataset\"\n",
    "yolo_weights_dir = \"kaggle/working/yolo_weights\"\n",
    "yolo_pretrained_weights = \"yolov8n.pt\"  # Path to pre-downloaded weights\n",
    "\n",
    "# Create weights directory if it doesn't exist\n",
    "os.makedirs(yolo_weights_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def fix_yaml_paths(yaml_path):\n",
    "    \"\"\"\n",
    "    Fix the paths in the YAML file to match the actual Kaggle directories\n",
    "    \n",
    "    Args:\n",
    "        yaml_path (str): Path to the original dataset YAML file\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the fixed YAML file\n",
    "    \"\"\"\n",
    "    print(f\"Fixing YAML paths in {yaml_path}\")\n",
    "    \n",
    "    # Read the original YAML\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        yaml_data = yaml.safe_load(f)\n",
    "    \n",
    "    # Update paths to use actual dataset location\n",
    "    if 'path' in yaml_data:\n",
    "        yaml_data['path'] = yolo_dataset_dir\n",
    "    \n",
    "    # Create a new fixed YAML in the working directory\n",
    "    fixed_yaml_path = \"kaggle/working/fixed_dataset.yaml\"\n",
    "    with open(fixed_yaml_path, 'w') as f:\n",
    "        yaml.dump(yaml_data, f)\n",
    "    \n",
    "    print(f\"Created fixed YAML at {fixed_yaml_path} with path: {yaml_data.get('path')}\")\n",
    "    return fixed_yaml_path\n",
    "\n",
    "\n",
    "\n",
    "def plot_dfl_loss_curve(run_dir):\n",
    "    \"\"\"\n",
    "    Plot the DFL loss curves for train and validation, marking the best model\n",
    "    \n",
    "    Args:\n",
    "        run_dir (str): Directory where the training results are stored\n",
    "    \"\"\"\n",
    "    # Path to the results CSV file\n",
    "    results_csv = os.path.join(run_dir, 'results.csv')\n",
    "    \n",
    "    if not os.path.exists(results_csv):\n",
    "        print(f\"Results file not found at {results_csv}\")\n",
    "        return\n",
    "    \n",
    "    # Read results CSV\n",
    "    results_df = pd.read_csv(results_csv)\n",
    "    \n",
    "    # Check if DFL loss columns exist\n",
    "    train_dfl_col = [col for col in results_df.columns if 'train/dfl_loss' in col]\n",
    "    val_dfl_col = [col for col in results_df.columns if 'val/dfl_loss' in col]\n",
    "    \n",
    "    if not train_dfl_col or not val_dfl_col:\n",
    "        print(\"DFL loss columns not found in results CSV\")\n",
    "        print(f\"Available columns: {results_df.columns.tolist()}\")\n",
    "        return\n",
    "    \n",
    "    train_dfl_col = train_dfl_col[0]\n",
    "    val_dfl_col = val_dfl_col[0]\n",
    "    \n",
    "    # Find the epoch with the best validation loss\n",
    "    best_epoch = results_df[val_dfl_col].idxmin()\n",
    "    best_val_loss = results_df.loc[best_epoch, val_dfl_col]\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot training and validation losses\n",
    "    plt.plot(results_df['epoch'], results_df[train_dfl_col], label='Train DFL Loss')\n",
    "    plt.plot(results_df['epoch'], results_df[val_dfl_col], label='Validation DFL Loss')\n",
    "    \n",
    "    # Mark the best model with a vertical line\n",
    "    plt.axvline(x=results_df.loc[best_epoch, 'epoch'], color='r', linestyle='--', \n",
    "                label=f'Best Model (Epoch {int(results_df.loc[best_epoch, \"epoch\"])}, Val Loss: {best_val_loss:.4f})')\n",
    "    \n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('DFL Loss')\n",
    "    plt.title('Training and Validation DFL Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Save the plot in the same directory as weights\n",
    "    plot_path = os.path.join(run_dir, 'dfl_loss_curve.png')\n",
    "    plt.savefig(plot_path)\n",
    "    \n",
    "    # Also save it to the working directory for easier access\n",
    "    plt.savefig(os.path.join('kaggle/working', 'dfl_loss_curve.png'))\n",
    "    \n",
    "    print(f\"Loss curve saved to {plot_path}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Return the best epoch info\n",
    "    return best_epoch, best_val_loss\n",
    "\n",
    "\n",
    "\n",
    "def train_yolo_model(yaml_path, pretrained_weights_path, epochs=30, batch_size=12, img_size=640):\n",
    "    \"\"\"\n",
    "    Train a YOLO model on the prepared dataset\n",
    "    \n",
    "    Args:\n",
    "        yaml_path (str): Path to the dataset YAML file\n",
    "        pretrained_weights_path (str): Path to pre-downloaded weights file\n",
    "        epochs (int): Number of training epochs\n",
    "        batch_size (int): Batch size for training\n",
    "        img_size (int): Image size for training\n",
    "    \"\"\"\n",
    "    print(f\"Loading pre-trained weights from: {pretrained_weights_path}\")\n",
    "    \n",
    "    # Load a pre-trained YOLOv8 model\n",
    "    model = YOLO(pretrained_weights_path)\n",
    "    \n",
    "    # Train the model with early stopping\n",
    "    results = model.train(\n",
    "        data=yaml_path,\n",
    "        epochs=epochs,\n",
    "        batch=batch_size,\n",
    "        imgsz=img_size,\n",
    "        project=\"yolo_weights_dir\",\n",
    "        name='motor_detector',\n",
    "        exist_ok=True,\n",
    "        patience=5,              # Early stopping if no improvement for 5 epochs\n",
    "        save_period=5,           # Save checkpoints every 5 epochs\n",
    "        val=True,                # Ensure validation is performed\n",
    "        verbose=True\n",
    "                                  # Show detailed output during training\n",
    "    )\n",
    "    \n",
    "    # Get the path to the run directory\n",
    "    run_dir = os.path.join(yolo_weights_dir, 'motor_detector')\n",
    "    \n",
    "    # Plot and save the loss curve\n",
    "    best_epoch_info = plot_dfl_loss_curve(run_dir)\n",
    "    \n",
    "    if best_epoch_info:\n",
    "        best_epoch, best_val_loss = best_epoch_info\n",
    "        print(f\"\\nBest model found at epoch {best_epoch} with validation DFL loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return model, results\n",
    "\n",
    "\n",
    "\n",
    "def predict_on_samples(model, num_samples=4):\n",
    "    \"\"\"\n",
    "    Run predictions on random validation samples and display results\n",
    "    \n",
    "    Args:\n",
    "        model: Trained YOLO model\n",
    "        num_samples (int): Number of random samples to test\n",
    "    \"\"\"\n",
    "    # Get validation images\n",
    "    val_dir = os.path.join(yolo_dataset_dir, 'images', 'val')\n",
    "    if not os.path.exists(val_dir):\n",
    "        print(f\"Validation directory not found at {val_dir}\")\n",
    "        # Try train directory instead if val doesn't exist\n",
    "        val_dir = os.path.join(yolo_dataset_dir, 'images', 'train')\n",
    "        print(f\"Using train directory for predictions instead: {val_dir}\")\n",
    "        \n",
    "    if not os.path.exists(val_dir):\n",
    "        print(\"No images directory found for predictions\")\n",
    "        return\n",
    "    \n",
    "    val_images = os.listdir(val_dir)\n",
    "    \n",
    "    if len(val_images) == 0:\n",
    "        print(\"No images found for prediction\")\n",
    "        return\n",
    "    \n",
    "    # Select random samples\n",
    "    num_samples = min(num_samples, len(val_images))\n",
    "    samples = random.sample(val_images, num_samples)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, img_file in enumerate(samples):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "            \n",
    "        img_path = os.path.join(val_dir, img_file)\n",
    "        \n",
    "        # Run prediction\n",
    "        results = model.predict(img_path, conf=0.25)[0]\n",
    "        \n",
    "        # Load and display the image\n",
    "        img = Image.open(img_path)\n",
    "        axes[i].imshow(np.array(img), cmap='gray')\n",
    "        \n",
    "        # Draw ground truth box if available (from filename)\n",
    "        try:\n",
    "            # This assumes your filenames contain coordinates in a specific format\n",
    "            parts = img_file.split('_')\n",
    "            y_part = [p for p in parts if p.startswith('y')]\n",
    "            x_part = [p for p in parts if p.startswith('x')]\n",
    "            \n",
    "            if y_part and x_part:\n",
    "                y_gt = int(y_part[0][1:])\n",
    "                x_gt = int(x_part[0][1:].split('.')[0])\n",
    "                \n",
    "                box_size = 24\n",
    "                rect_gt = Rectangle((x_gt - box_size//2, y_gt - box_size//2), \n",
    "                              box_size, box_size, \n",
    "                              linewidth=1, edgecolor='g', facecolor='none')\n",
    "                axes[i].add_patch(rect_gt)\n",
    "        except:\n",
    "            pass  # Skip ground truth if parsing fails\n",
    "        \n",
    "        # Draw predicted boxes (red)\n",
    "        if len(results.boxes) > 0:\n",
    "            boxes = results.boxes.xyxy.cpu().numpy()\n",
    "            confs = results.boxes.conf.cpu().numpy()\n",
    "            \n",
    "            for box, conf in zip(boxes, confs):\n",
    "                x1, y1, x2, y2 = box\n",
    "                rect_pred = Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                     linewidth=1, edgecolor='r', facecolor='none')\n",
    "                axes[i].add_patch(rect_pred)\n",
    "                axes[i].text(x1, y1-5, f'{conf:.2f}', color='red')\n",
    "        \n",
    "        axes[i].set_title(f\"Image: {img_file}\\nGround Truth (green) vs Prediction (red)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the predictions plot\n",
    "    plt.savefig(os.path.join('kaggle/working', 'predictions.png'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Check and create a dataset YAML if needed\n",
    "def prepare_dataset():\n",
    "    \"\"\"\n",
    "    Check if dataset exists and create a proper YAML if needed\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the YAML file to use for training\n",
    "    \"\"\"\n",
    "    # Check if images exist\n",
    "    train_images_dir = os.path.join(yolo_dataset_dir, 'images', 'train')\n",
    "    val_images_dir = os.path.join(yolo_dataset_dir, 'images', 'val')\n",
    "    train_labels_dir = os.path.join(yolo_dataset_dir, 'labels', 'train')\n",
    "    val_labels_dir = os.path.join(yolo_dataset_dir, 'labels', 'val')\n",
    "    \n",
    "    # Print directory existence status\n",
    "    print(f\"Directory status:\")\n",
    "    print(f\"- Train images dir exists: {os.path.exists(train_images_dir)}\")\n",
    "    print(f\"- Val images dir exists: {os.path.exists(val_images_dir)}\")\n",
    "    print(f\"- Train labels dir exists: {os.path.exists(train_labels_dir)}\")\n",
    "    print(f\"- Val labels dir exists: {os.path.exists(val_labels_dir)}\")\n",
    "    \n",
    "    # Check for original YAML file\n",
    "    original_yaml_path = os.path.join(yolo_dataset_dir, 'dataset.yaml')\n",
    "    \n",
    "    if os.path.exists(original_yaml_path):\n",
    "        print(f\"Found original dataset.yaml at {original_yaml_path}\")\n",
    "        # Fix the paths in the YAML\n",
    "        return fix_yaml_paths(original_yaml_path)\n",
    "    else:\n",
    "        print(f\"Original dataset.yaml not found, creating a new one\")\n",
    "        \n",
    "        # Create a new YAML file\n",
    "        yaml_data = {\n",
    "            'path': yolo_dataset_dir,\n",
    "            'train': 'images/train',\n",
    "            'val': 'images/train' if not os.path.exists(val_images_dir) else 'images/val',\n",
    "            'names': {0: 'motor'}\n",
    "        }\n",
    "        \n",
    "        new_yaml_path = \"kaggle/working/dataset.yaml\"\n",
    "        with open(new_yaml_path, 'w') as f:\n",
    "            yaml.dump(yaml_data, f)\n",
    "            \n",
    "        print(f\"Created new YAML at {new_yaml_path}\")\n",
    "        return new_yaml_path\n",
    "\n",
    "\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    print(\"Starting YOLO training process...\")\n",
    "    \n",
    "    # Prepare dataset and get YAML path\n",
    "    yaml_path = prepare_dataset()\n",
    "    print(f\"Using YAML file: {yaml_path}\")\n",
    "    \n",
    "    # Print YAML file contents\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        yaml_content = f.read()\n",
    "    print(f\"YAML file contents:\\n{yaml_content}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nStarting YOLO training...\")\n",
    "    model, results = train_yolo_model(\n",
    "        yaml_path,\n",
    "        pretrained_weights_path=yolo_pretrained_weights,\n",
    "        epochs=30  # Using 30 epochs instead of 100 for faster training\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining complete!\")\n",
    "    \n",
    "    # Run predictions\n",
    "    print(\"\\nRunning predictions on sample images...\")\n",
    "    predict_on_samples(model, num_samples=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcarlofinnegan\u001b[0m (\u001b[33mtraintest\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flag_motor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
