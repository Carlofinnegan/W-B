{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm  \n",
    "import os\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import time\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define global constants for dataset directories\n",
    "DATA_DIR = '/kaggle/input/byu-locating-bacterial-flagellar-motors-2025'\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, 'train_labels.csv')\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "TEST_DIR = os.path.join(DATA_DIR, 'test')\n",
    "OUTPUT_DIR = './'\n",
    "MODEL_DIR = './models'\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Set device: Use GPU if available; otherwise, fall back to CPU\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motors in the dataset: 831\n",
      "Found 362 unique tomograms with motors\n",
      "Split: 289 tomograms for training, 73 tomograms for validation\n",
      "Will process approximately 3267 slices for training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95bb0bd2614449197d6fc33471e997b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing training motors:   0%|          | 0/363 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will process approximately 792 slices for validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd41d2b38883480da9e69682a9df7ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing validation motors:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Summary:\n",
      "- Train set: 289 tomograms, 363 motors, 3262 slices\n",
      "- Validation set: 73 tomograms, 88 motors, 792 slices\n",
      "- Total: 362 tomograms, 451 motors, 4054 slices\n",
      "\n",
      "Preprocessing Complete:\n",
      "- Training data: 289 tomograms, 363 motors, 3262 slices\n",
      "- Validation data: 73 tomograms, 88 motors, 792 slices\n",
      "- Dataset directory: kaggle/working/yolo_dataset\n",
      "- YAML configuration: kaggle/working/yolo_dataset/dataset.yaml\n",
      "\n",
      "Ready for YOLO training!\n"
     ]
    }
   ],
   "source": [
    "# Define YOLO dataset structure and parameters\n",
    "data_path = \"kaggle/input/byu-locating-bacterial-flagellar-motors-2025/\"\n",
    "train_dir = os.path.join(data_path, \"train\")\n",
    "\n",
    "# Output directories for YOLO dataset (adjust as needed)\n",
    "yolo_dataset_dir = \"kaggle/working/yolo_dataset\"\n",
    "yolo_images_train = os.path.join(yolo_dataset_dir, \"images\", \"train\")\n",
    "yolo_images_val = os.path.join(yolo_dataset_dir, \"images\", \"val\")\n",
    "yolo_labels_train = os.path.join(yolo_dataset_dir, \"labels\", \"train\")\n",
    "yolo_labels_val = os.path.join(yolo_dataset_dir, \"labels\", \"val\")\n",
    "\n",
    "# Create necessary directories\n",
    "for dir_path in [yolo_images_train, yolo_images_val, yolo_labels_train, yolo_labels_val]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Define constants for processing\n",
    "TRUST = 4       # Number of slices above and below center slice (total slices = 2*TRUST + 1)\n",
    "BOX_SIZE = 24   # Bounding box size (in pixels)\n",
    "TRAIN_SPLIT = 0.8  # 80% training, 20% validation\n",
    "\n",
    "# Define a helper function for image normalization using percentile-based contrast enhancement.\n",
    "def normalize_slice(slice_data):\n",
    "    \"\"\"\n",
    "    Normalize slice data using the 2nd and 98th percentiles.\n",
    "    \n",
    "    Args:\n",
    "        slice_data (numpy.array): Input image slice.\n",
    "    \n",
    "    Returns:\n",
    "        np.uint8: Normalized image in the range [0, 255].\n",
    "    \"\"\"\n",
    "    p2 = np.percentile(slice_data, 2)\n",
    "    p98 = np.percentile(slice_data, 98)\n",
    "    clipped_data = np.clip(slice_data, p2, p98)\n",
    "    normalized = 255 * (clipped_data - p2) / (p98 - p2)\n",
    "    return np.uint8(normalized)\n",
    "\n",
    "# Define the preprocessing function to extract slices, normalize, and generate YOLO annotations.\n",
    "def prepare_yolo_dataset(trust=TRUST, train_split=TRAIN_SPLIT):\n",
    "    \"\"\"\n",
    "    Extract slices containing motors and save images with corresponding YOLO annotations.\n",
    "    \n",
    "    Steps:\n",
    "    - Load the motor labels.\n",
    "    - Perform a train/validation split by tomogram.\n",
    "    - For each motor, extract slices in a range (Â± trust parameter).\n",
    "    - Normalize each slice and save it.\n",
    "    - Generate YOLO format bounding box annotations with a fixed box size.\n",
    "    - Create a YAML configuration file for YOLO training.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A summary containing dataset statistics and file paths.\n",
    "    \"\"\"\n",
    "    # Load the labels CSV\n",
    "    labels_df = pd.read_csv(os.path.join(data_path, \"train_labels.csv\"))\n",
    "    \n",
    "    total_motors = labels_df['Number of motors'].sum()\n",
    "    print(f\"Total number of motors in the dataset: {total_motors}\")\n",
    "    \n",
    "    # Consider only tomograms with at least one motor\n",
    "    tomo_df = labels_df[labels_df['Number of motors'] > 0].copy()\n",
    "    unique_tomos = tomo_df['tomo_id'].unique()\n",
    "    print(f\"Found {len(unique_tomos)} unique tomograms with motors\")\n",
    "    \n",
    "    # Shuffle and split tomograms into train and validation sets\n",
    "    np.random.shuffle(unique_tomos)\n",
    "    split_idx = int(len(unique_tomos) * train_split)\n",
    "    train_tomos = unique_tomos[:split_idx]\n",
    "    val_tomos = unique_tomos[split_idx:]\n",
    "    print(f\"Split: {len(train_tomos)} tomograms for training, {len(val_tomos)} tomograms for validation\")\n",
    "    \n",
    "    # Helper function to process a list of tomograms\n",
    "    def process_tomogram_set(tomogram_ids, images_dir, labels_dir, set_name):\n",
    "        motor_counts = []\n",
    "        for tomo_id in tomogram_ids:\n",
    "            # Get motor annotations for the current tomogram\n",
    "            tomo_motors = labels_df[labels_df['tomo_id'] == tomo_id]\n",
    "            for _, motor in tomo_motors.iterrows():\n",
    "                if pd.isna(motor['Motor axis 0']):\n",
    "                    continue\n",
    "                motor_counts.append(\n",
    "                    (tomo_id, \n",
    "                     int(motor['Motor axis 0']), \n",
    "                     int(motor['Motor axis 1']), \n",
    "                     int(motor['Motor axis 2']),\n",
    "                     int(motor['Array shape (axis 0)']))\n",
    "                )\n",
    "        \n",
    "        print(f\"Will process approximately {len(motor_counts) * (2 * trust + 1)} slices for {set_name}\")\n",
    "        processed_slices = 0\n",
    "        \n",
    "        # Loop over each motor annotation\n",
    "        for tomo_id, z_center, y_center, x_center, z_max in tqdm(motor_counts, desc=f\"Processing {set_name} motors\"):\n",
    "            z_min = max(0, z_center - trust)\n",
    "            z_max_bound = min(z_max - 1, z_center + trust)\n",
    "            for z in range(z_min, z_max_bound + 1):\n",
    "                # Create the slice filename and source path\n",
    "                slice_filename = f\"slice_{z:04d}.jpg\"\n",
    "                src_path = os.path.join(train_dir, tomo_id, slice_filename)\n",
    "                if not os.path.exists(src_path):\n",
    "                    print(f\"Warning: {src_path} does not exist, skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                # Load, normalize, and save the image slice\n",
    "                img = Image.open(src_path)\n",
    "                img_array = np.array(img)\n",
    "                normalized_img = normalize_slice(img_array)\n",
    "                dest_filename = f\"{tomo_id}_z{z:04d}_y{y_center:04d}_x{x_center:04d}.jpg\"\n",
    "                dest_path = os.path.join(images_dir, dest_filename)\n",
    "                Image.fromarray(normalized_img).save(dest_path)\n",
    "                \n",
    "                # Prepare YOLO bounding box annotation (normalized values)\n",
    "                img_width, img_height = img.size\n",
    "                x_center_norm = x_center / img_width\n",
    "                y_center_norm = y_center / img_height\n",
    "                box_width_norm = BOX_SIZE / img_width\n",
    "                box_height_norm = BOX_SIZE / img_height\n",
    "                label_path = os.path.join(labels_dir, dest_filename.replace('.jpg', '.txt'))\n",
    "                with open(label_path, 'w') as f:\n",
    "                    f.write(f\"0 {x_center_norm} {y_center_norm} {box_width_norm} {box_height_norm}\\n\")\n",
    "                \n",
    "                processed_slices += 1\n",
    "        \n",
    "        return processed_slices, len(motor_counts)\n",
    "    \n",
    "    # Process training tomograms\n",
    "    train_slices, train_motors = process_tomogram_set(train_tomos, yolo_images_train, yolo_labels_train, \"training\")\n",
    "    # Process validation tomograms\n",
    "    val_slices, val_motors = process_tomogram_set(val_tomos, yolo_images_val, yolo_labels_val, \"validation\")\n",
    "    \n",
    "    # Generate YAML configuration for YOLO training\n",
    "    yaml_content = {\n",
    "        'path': yolo_dataset_dir,\n",
    "        'train': 'images/train',\n",
    "        'val': 'images/val',\n",
    "        'names': {0: 'motor'}\n",
    "    }\n",
    "    with open(os.path.join(yolo_dataset_dir, 'dataset.yaml'), 'w') as f:\n",
    "        yaml.dump(yaml_content, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"\\nProcessing Summary:\")\n",
    "    print(f\"- Train set: {len(train_tomos)} tomograms, {train_motors} motors, {train_slices} slices\")\n",
    "    print(f\"- Validation set: {len(val_tomos)} tomograms, {val_motors} motors, {val_slices} slices\")\n",
    "    print(f\"- Total: {len(train_tomos) + len(val_tomos)} tomograms, {train_motors + val_motors} motors, {train_slices + val_slices} slices\")\n",
    "    \n",
    "    return {\n",
    "        \"dataset_dir\": yolo_dataset_dir,\n",
    "        \"yaml_path\": os.path.join(yolo_dataset_dir, 'dataset.yaml'),\n",
    "        \"train_tomograms\": len(train_tomos),\n",
    "        \"val_tomograms\": len(val_tomos),\n",
    "        \"train_motors\": train_motors,\n",
    "        \"val_motors\": val_motors,\n",
    "        \"train_slices\": train_slices,\n",
    "        \"val_slices\": val_slices\n",
    "    }\n",
    "\n",
    "# Run the preprocessing\n",
    "summary = prepare_yolo_dataset(TRUST)\n",
    "print(f\"\\nPreprocessing Complete:\")\n",
    "print(f\"- Training data: {summary['train_tomograms']} tomograms, {summary['train_motors']} motors, {summary['train_slices']} slices\")\n",
    "print(f\"- Validation data: {summary['val_tomograms']} tomograms, {summary['val_motors']} motors, {summary['val_slices']} slices\")\n",
    "print(f\"- Dataset directory: {summary['dataset_dir']}\")\n",
    "print(f\"- YAML configuration: {summary['yaml_path']}\")\n",
    "print(\"\\nReady for YOLO training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcarlofinnegan\u001b[0m (\u001b[33mtraintest\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3262 images in training set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|ââââââââââ| 3262/3262 [01:11<00:00, 45.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation complete!\n",
      "Augmented images saved to: kaggle/working/yolo_dataset/images/train_augmented\n",
      "Augmented labels saved to: kaggle/working/yolo_dataset/labels/train_augmented\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_augmentations(dataset_root):\n",
    "    \"\"\"\n",
    "    Apply 8 different transformations to images in a YOLO dataset and update labels accordingly.\n",
    "    \n",
    "    Transformations:\n",
    "    1. Vertical flip\n",
    "    2. 90Â° rotation\n",
    "    3. Vertical flip + 90Â° rotation\n",
    "    4. 180Â° rotation\n",
    "    5. Vertical flip + 180Â° rotation\n",
    "    6. 270Â° rotation\n",
    "    7. Vertical flip + 270Â° rotation\n",
    "    8. No transformation (original)\n",
    "    \"\"\"\n",
    "    # Define directories\n",
    "    images_dir = os.path.join(dataset_root, \"images\", \"train\")\n",
    "    labels_dir = os.path.join(dataset_root, \"labels\", \"train\")\n",
    "    \n",
    "    # Create augmented directories\n",
    "    augmented_images_dir = os.path.join(dataset_root, \"images\", \"train_augmented\")\n",
    "    augmented_labels_dir = os.path.join(dataset_root, \"labels\", \"train_augmented\")\n",
    "    \n",
    "    os.makedirs(augmented_images_dir, exist_ok=True)\n",
    "    os.makedirs(augmented_labels_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all image files\n",
    "    image_files = []\n",
    "    for ext in ['.jpg', '.jpeg', '.png', '.tif', '.tiff', '.bmp']:\n",
    "        image_files.extend(list(Path(images_dir).glob(f'*{ext}')))\n",
    "        image_files.extend(list(Path(images_dir).glob(f'*{ext.upper()}')))\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images in training set.\")\n",
    "    \n",
    "    # Process each image and its corresponding label\n",
    "    for img_path in tqdm(image_files, desc=\"Processing images\"):\n",
    "        # Get base filename without extension\n",
    "        base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        ext = os.path.splitext(img_path)[1]\n",
    "        \n",
    "        # Find corresponding label file\n",
    "        label_path = os.path.join(labels_dir, f\"{base_name}.txt\")\n",
    "        if not os.path.exists(label_path):\n",
    "            print(f\"Warning: No label file found for {img_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Read label file\n",
    "        with open(label_path, 'r') as f:\n",
    "            label_lines = f.readlines()\n",
    "        \n",
    "        # Load image\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            width, height = img.size\n",
    "            \n",
    "            # Process transformations\n",
    "            transformations = [\n",
    "                (\"original\", lambda i: i, lambda x, y, w, h: (x, y, w, h)),\n",
    "                (\"vflip\", lambda i: i.transpose(Image.FLIP_TOP_BOTTOM), \n",
    "                 lambda x, y, w, h: (x, 1.0-y, w, h)),\n",
    "                (\"rot90\", lambda i: i.transpose(Image.ROTATE_90), \n",
    "                 lambda x, y, w, h: (y, 1.0-x, h, w)),\n",
    "                (\"vflip_rot90\", lambda i: i.transpose(Image.FLIP_TOP_BOTTOM).transpose(Image.ROTATE_90), \n",
    "                 lambda x, y, w, h: (1.0-y, 1.0-x, h, w)),\n",
    "                (\"rot180\", lambda i: i.transpose(Image.ROTATE_180), \n",
    "                 lambda x, y, w, h: (1.0-x, 1.0-y, w, h)),\n",
    "                (\"vflip_rot180\", lambda i: i.transpose(Image.FLIP_TOP_BOTTOM).transpose(Image.ROTATE_180), \n",
    "                 lambda x, y, w, h: (1.0-x, y, w, h)),\n",
    "                (\"rot270\", lambda i: i.transpose(Image.ROTATE_270), \n",
    "                 lambda x, y, w, h: (1.0-y, x, h, w)),\n",
    "                (\"vflip_rot270\", lambda i: i.transpose(Image.FLIP_TOP_BOTTOM).transpose(Image.ROTATE_270), \n",
    "                 lambda x, y, w, h: (y, x, h, w))\n",
    "            ]\n",
    "            \n",
    "            for name, img_transform, bbox_transform in transformations:\n",
    "                # Transform and save image\n",
    "                transformed_img = img_transform(img)\n",
    "                transformed_img.save(os.path.join(augmented_images_dir, f\"{base_name}_{name}{ext}\"))\n",
    "                \n",
    "                # Transform and save label\n",
    "                transformed_labels = []\n",
    "                for line in label_lines:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:  # class x y w h format\n",
    "                        class_id = parts[0]\n",
    "                        x, y, w, h = map(float, parts[1:5])\n",
    "                        \n",
    "                        # For rotations that swap width and height\n",
    "                        if \"rot90\" in name or \"rot270\" in name:\n",
    "                            new_x, new_y, new_w, new_h = bbox_transform(x, y, w, h)\n",
    "                        else:\n",
    "                            new_x, new_y, new_w, new_h = bbox_transform(x, y, w, h)\n",
    "                        \n",
    "                        transformed_labels.append(f\"{class_id} {new_x:.6f} {new_y:.6f} {new_w:.6f} {new_h:.6f}\")\n",
    "                    else:\n",
    "                        transformed_labels.append(line.strip())  # Keep unchanged if not in standard format\n",
    "                \n",
    "                # Write transformed label file\n",
    "                with open(os.path.join(augmented_labels_dir, f\"{base_name}_{name}.txt\"), 'w') as f:\n",
    "                    f.write('\\n'.join(transformed_labels))\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "    \n",
    "    print(\"Augmentation complete!\")\n",
    "    print(f\"Augmented images saved to: {augmented_images_dir}\")\n",
    "    print(f\"Augmented labels saved to: {augmented_labels_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual dataset root directory\n",
    "    dataset_root = \"kaggle/working/yolo_dataset\"\n",
    "    create_augmentations(dataset_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/carlo/Documents/Github/W&B/wandb/run-20250313_190307-6capl6uq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/traintest/BYU_Comp/runs/6capl6uq' target=\"_blank\">smooth-morning-9</a></strong> to <a href='https://wandb.ai/traintest/BYU_Comp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/traintest/BYU_Comp' target=\"_blank\">https://wandb.ai/traintest/BYU_Comp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/traintest/BYU_Comp/runs/6capl6uq' target=\"_blank\">https://wandb.ai/traintest/BYU_Comp/runs/6capl6uq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting YOLO training process...\n",
      "Directory status:\n",
      "- Train images dir exists: True\n",
      "- Val images dir exists: True\n",
      "- Train labels dir exists: True\n",
      "- Val labels dir exists: True\n",
      "Original dataset.yaml not found, creating a new one\n",
      "Created new YAML at kaggle/working/dataset.yaml\n",
      "Using YAML file: kaggle/working/dataset.yaml\n",
      "YAML file contents:\n",
      "names:\n",
      "  0: motor\n",
      "path: kaggle/working/yolo_dataset\n",
      "train: images/train_augmented\n",
      "val: images/val\n",
      "\n",
      "\n",
      "Starting YOLO training...\n",
      "Loading pre-trained weights from: yolov8n.pt\n",
      "New https://pypi.org/project/ultralytics/8.3.89 available ð Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.88 ð Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3070, 7971MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=kaggle/working/dataset.yaml, epochs=30, time=None, patience=20, batch=12, imgsz=640, save=True, save_period=5, cache=False, device=None, workers=8, project=yolo_weights_dir, name=motor_detector, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=yolo_weights_dir/motor_detector\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed â\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/carlo/Documents/Github/W&B/kaggle/working/yolo_dataset/labels/train_augmented.cache... 26096 images, 0 backgrounds, 0 corrupt: 100%|ââââââââââ| 26096/26096 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/carlo/Documents/Github/W&B/kaggle/working/yolo_dataset/labels/val.cache... 792 images, 0 backgrounds, 0 corrupt: 100%|ââââââââââ| 792/792 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to yolo_weights_dir/motor_detector/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.00046875), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1myolo_weights_dir/motor_detector\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/30      1.53G      2.861      6.025      1.158         10        640: 100%|ââââââââââ| 2175/2175 [02:33<00:00, 14.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:01<00:00, 17.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.621      0.652      0.651      0.223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/30      1.85G      2.122      1.587     0.9556         12        640: 100%|ââââââââââ| 2175/2175 [02:24<00:00, 15.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:01<00:00, 18.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.749      0.694      0.735      0.268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/30      1.85G      2.024      1.439     0.9392         11        640: 100%|ââââââââââ| 2175/2175 [02:22<00:00, 15.24it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:01<00:00, 18.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.663      0.588      0.587      0.173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/30      1.85G      1.925      1.356     0.9194         11        640: 100%|ââââââââââ| 2175/2175 [02:16<00:00, 15.89it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:01<00:00, 17.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.811      0.793      0.801      0.314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/30      1.85G      1.782      1.225     0.8966         10        640: 100%|ââââââââââ| 2175/2175 [02:18<00:00, 15.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:01<00:00, 18.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.801      0.755      0.799      0.323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/30      1.86G       1.67      1.109     0.8818         10        640: 100%|ââââââââââ| 2175/2175 [02:16<00:00, 15.94it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:01<00:00, 18.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.744      0.734      0.703      0.239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/30      1.86G      1.601      1.061     0.8726         12        640: 100%|ââââââââââ| 2175/2175 [02:29<00:00, 14.57it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:02<00:00, 14.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.789      0.763       0.77      0.252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/30      1.86G      1.547      1.017     0.8646         10        640: 100%|ââââââââââ| 2175/2175 [02:41<00:00, 13.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:02<00:00, 14.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.813      0.754      0.788      0.288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/30      1.86G      1.507     0.9705      0.859         13        640: 100%|ââââââââââ| 2175/2175 [02:41<00:00, 13.49it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:02<00:00, 14.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.807      0.813      0.814      0.333\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      10/30      1.86G      1.456     0.9355     0.8526         14        640: 100%|ââââââââââ| 2175/2175 [02:40<00:00, 13.56it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:02<00:00, 14.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.857      0.847      0.905      0.439\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      11/30      1.86G      1.407     0.8977     0.8493          7        640: 100%|ââââââââââ| 2175/2175 [02:33<00:00, 14.19it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:02<00:00, 16.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.871       0.82       0.88      0.387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/30      1.86G      1.369     0.8668     0.8424         10        640: 100%|ââââââââââ| 2175/2175 [02:28<00:00, 14.63it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:02<00:00, 16.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.813      0.823      0.857      0.371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/30      1.86G      1.336     0.8382     0.8408         12        640: 100%|ââââââââââ| 2175/2175 [02:27<00:00, 14.70it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:02<00:00, 16.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.864      0.843      0.879      0.403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/30      1.86G      1.292      0.808     0.8361          8        640: 100%|ââââââââââ| 2175/2175 [02:28<00:00, 14.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:02<00:00, 16.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792       0.85      0.831      0.857      0.381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/30      1.86G      1.262     0.7877     0.8305         10        640: 100%|ââââââââââ| 2175/2175 [02:28<00:00, 14.69it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:02<00:00, 16.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.829      0.861      0.893      0.403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/30      1.86G      1.225     0.7591     0.8275         10        640: 100%|ââââââââââ| 2175/2175 [02:35<00:00, 14.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:02<00:00, 15.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.839      0.843      0.882      0.435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/30      1.86G      1.196     0.7447      0.825         10        640: 100%|ââââââââââ| 2175/2175 [02:37<00:00, 13.83it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:02<00:00, 14.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.831      0.857      0.876      0.397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/30      1.86G      1.164     0.7231     0.8224          8        640: 100%|ââââââââââ| 2175/2175 [02:38<00:00, 13.70it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:01<00:00, 17.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.836      0.829       0.87      0.396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/30      1.86G      1.131     0.7022     0.8206         11        640: 100%|ââââââââââ| 2175/2175 [02:28<00:00, 14.62it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:02<00:00, 15.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.828      0.865      0.875      0.391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/30      1.86G      1.108     0.6896      0.816          9        640: 100%|ââââââââââ| 2175/2175 [02:25<00:00, 14.97it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:01<00:00, 16.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.855      0.838      0.882      0.416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/30      1.86G     0.9773     0.6077      0.815          6        640: 100%|ââââââââââ| 2175/2175 [02:36<00:00, 13.94it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:02<00:00, 13.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.831      0.861      0.885      0.408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/30      1.86G     0.9242     0.5755     0.8095          8        640: 100%|ââââââââââ| 2175/2175 [02:41<00:00, 13.47it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:02<00:00, 14.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.831      0.866       0.88      0.406\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      23/30      1.86G     0.8907     0.5572      0.805          7        640: 100%|ââââââââââ| 2175/2175 [02:40<00:00, 13.53it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:02<00:00, 14.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.834      0.851      0.885      0.407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/30      1.86G     0.8522     0.5323     0.8035          8        640: 100%|ââââââââââ| 2175/2175 [02:40<00:00, 13.56it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:02<00:00, 14.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.848      0.859      0.898      0.423\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      25/30      1.86G     0.8246     0.5153     0.8004          8        640: 100%|ââââââââââ| 2175/2175 [02:33<00:00, 14.19it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:02<00:00, 15.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.858      0.848      0.889      0.422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/30      1.86G     0.7938     0.4986     0.7963          8        640: 100%|ââââââââââ| 2175/2175 [02:33<00:00, 14.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:02<00:00, 15.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.856      0.854      0.885      0.418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/30      1.86G     0.7692      0.487     0.7953          8        640: 100%|ââââââââââ| 2175/2175 [02:37<00:00, 13.82it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:02<00:00, 16.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.845      0.856      0.886      0.414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/30      1.86G     0.7421     0.4678     0.7924          7        640: 100%|ââââââââââ| 2175/2175 [02:33<00:00, 14.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:02<00:00, 16.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.846      0.859      0.889      0.418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/30      1.86G     0.7045     0.4527     0.7901          8        640: 100%|ââââââââââ| 2175/2175 [02:33<00:00, 14.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:02<00:00, 16.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.846      0.863      0.891       0.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/30      1.86G     0.6907      0.445     0.7888          8        640: 100%|ââââââââââ| 2175/2175 [02:33<00:00, 14.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:02<00:00, 16.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.842      0.864      0.891      0.417\n",
      "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 20 epochs. Best results observed at epoch 10, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=20) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "30 epochs completed in 1.285 hours.\n",
      "Optimizer stripped from yolo_weights_dir/motor_detector/weights/last.pt, 6.3MB\n",
      "Optimizer stripped from yolo_weights_dir/motor_detector/weights/best.pt, 6.3MB\n",
      "\n",
      "Validating yolo_weights_dir/motor_detector/weights/best.pt...\n",
      "Ultralytics 8.3.88 ð Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3070, 7971MiB)\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|ââââââââââ| 33/33 [00:02<00:00, 15.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        792        792      0.857      0.847      0.905       0.44\n",
      "Speed: 0.1ms preprocess, 1.2ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1myolo_weights_dir/motor_detector\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>ââââââââââââââââââââââââââââââ</td></tr><tr><td>lr/pg1</td><td>ââââââââââââââââââââââââââââââ</td></tr><tr><td>lr/pg2</td><td>ââââââââââââââââââââââââââââââ</td></tr><tr><td>metrics/mAP50(B)</td><td>ââââââââââââââââââââââââââââââ</td></tr><tr><td>metrics/mAP50-95(B)</td><td>ââââââââââââââââââââââââââââââ</td></tr><tr><td>metrics/precision(B)</td><td>ââââââââââââââââââââââââââââââ</td></tr><tr><td>metrics/recall(B)</td><td>ââââââââââââââââââââââââââââââ</td></tr><tr><td>model/GFLOPs</td><td>â</td></tr><tr><td>model/parameters</td><td>â</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>â</td></tr><tr><td>train/box_loss</td><td>ââââââââââââââââââââââââââââââ</td></tr><tr><td>train/cls_loss</td><td>ââââââââââââââââââââââââââââââ</td></tr><tr><td>train/dfl_loss</td><td>ââââââââââââââââââââââââââââââ</td></tr><tr><td>val/box_loss</td><td>ââââââââââââââââââââââââââââââ</td></tr><tr><td>val/cls_loss</td><td>ââââââââââââââââââââââââââââââ</td></tr><tr><td>val/dfl_loss</td><td>ââââââââââââââââââââââââââââââ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>0.00043</td></tr><tr><td>lr/pg1</td><td>0.00043</td></tr><tr><td>lr/pg2</td><td>0.00043</td></tr><tr><td>metrics/mAP50(B)</td><td>0.90498</td></tr><tr><td>metrics/mAP50-95(B)</td><td>0.43963</td></tr><tr><td>metrics/precision(B)</td><td>0.85689</td></tr><tr><td>metrics/recall(B)</td><td>0.84675</td></tr><tr><td>model/GFLOPs</td><td>8.194</td></tr><tr><td>model/parameters</td><td>3011043</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>1.209</td></tr><tr><td>train/box_loss</td><td>0.69069</td></tr><tr><td>train/cls_loss</td><td>0.44503</td></tr><tr><td>train/dfl_loss</td><td>0.78883</td></tr><tr><td>val/box_loss</td><td>1.94921</td></tr><tr><td>val/cls_loss</td><td>0.86618</td></tr><tr><td>val/dfl_loss</td><td>0.9159</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smooth-morning-9</strong> at: <a href='https://wandb.ai/traintest/BYU_Comp/runs/6capl6uq' target=\"_blank\">https://wandb.ai/traintest/BYU_Comp/runs/6capl6uq</a><br> View project at: <a href='https://wandb.ai/traintest/BYU_Comp' target=\"_blank\">https://wandb.ai/traintest/BYU_Comp</a><br>Synced 5 W&B file(s), 24 media file(s), 10 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250313_190307-6capl6uq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss curve saved to kaggle/working/yolo_weights/motor_detector/dfl_loss_curve.png\n",
      "\n",
      "Best model found at epoch 12 with validation DFL loss: 0.9020\n",
      "\n",
      "Training complete!\n",
      "\n",
      "Running predictions on sample images...\n",
      "\n",
      "image 1/1 /home/carlo/Documents/Github/W&B/kaggle/working/yolo_dataset/images/val/tomo_6f83d4_z0120_y0424_x0293.jpg: 640x640 1 motor, 2.3ms\n",
      "Speed: 1.4ms preprocess, 2.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /home/carlo/Documents/Github/W&B/kaggle/working/yolo_dataset/images/val/tomo_a37a5c_z0165_y0748_x0709.jpg: 640x640 1 motor, 2.8ms\n",
      "Speed: 2.2ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /home/carlo/Documents/Github/W&B/kaggle/working/yolo_dataset/images/val/tomo_37dd38_z0187_y0303_x0593.jpg: 640x640 (no detections), 2.9ms\n",
      "Speed: 1.7ms preprocess, 2.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /home/carlo/Documents/Github/W&B/kaggle/working/yolo_dataset/images/val/tomo_3e7407_z0267_y0349_x0736.jpg: 640x640 1 motor, 3.1ms\n",
      "Speed: 2.1ms preprocess, 3.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from ultralytics import YOLO\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import json\n",
    "import wandb\n",
    "from wandb.integration.ultralytics import add_wandb_callback\n",
    "\n",
    "\n",
    "\n",
    "wandb.init(project=\"BYU_Comp\", job_type=\"training\")\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define paths for Kaggle environment\n",
    "yolo_dataset_dir = \"kaggle/working/yolo_dataset\"\n",
    "yolo_weights_dir = \"kaggle/working/yolo_weights\"\n",
    "yolo_pretrained_weights = \"yolov8n.pt\"  # Path to pre-downloaded weights\n",
    "\n",
    "# Create weights directory if it doesn't exist\n",
    "os.makedirs(yolo_weights_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "def fix_yaml_paths(yaml_path):\n",
    "    \"\"\"\n",
    "    Fix the paths in the YAML file to match the actual Kaggle directories\n",
    "    \n",
    "    Args:\n",
    "        yaml_path (str): Path to the original dataset YAML file\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the fixed YAML file\n",
    "    \"\"\"\n",
    "    print(f\"Fixing YAML paths in {yaml_path}\")\n",
    "    \n",
    "    # Read the original YAML\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        yaml_data = yaml.safe_load(f)\n",
    "    \n",
    "    # Update paths to use actual dataset location\n",
    "    if 'path' in yaml_data:\n",
    "        yaml_data['path'] = yolo_dataset_dir\n",
    "    \n",
    "    # Create a new fixed YAML in the working directory\n",
    "    fixed_yaml_path = \"kaggle/working/fixed_dataset.yaml\"\n",
    "    with open(fixed_yaml_path, 'w') as f:\n",
    "        yaml.dump(yaml_data, f)\n",
    "    \n",
    "    print(f\"Created fixed YAML at {fixed_yaml_path} with path: {yaml_data.get('path')}\")\n",
    "    return fixed_yaml_path\n",
    "\n",
    "\n",
    "\n",
    "def plot_dfl_loss_curve(run_dir):\n",
    "    \"\"\"\n",
    "    Plot the DFL loss curves for train and validation, marking the best model\n",
    "    \n",
    "    Args:\n",
    "        run_dir (str): Directory where the training results are stored\n",
    "    \"\"\"\n",
    "    # Path to the results CSV file\n",
    "    results_csv = os.path.join(run_dir, 'results.csv')\n",
    "    \n",
    "    if not os.path.exists(results_csv):\n",
    "        print(f\"Results file not found at {results_csv}\")\n",
    "        return\n",
    "    \n",
    "    # Read results CSV\n",
    "    results_df = pd.read_csv(results_csv)\n",
    "    \n",
    "    # Check if DFL loss columns exist\n",
    "    train_dfl_col = [col for col in results_df.columns if 'train/dfl_loss' in col]\n",
    "    val_dfl_col = [col for col in results_df.columns if 'val/dfl_loss' in col]\n",
    "    \n",
    "    if not train_dfl_col or not val_dfl_col:\n",
    "        print(\"DFL loss columns not found in results CSV\")\n",
    "        print(f\"Available columns: {results_df.columns.tolist()}\")\n",
    "        return\n",
    "    \n",
    "    train_dfl_col = train_dfl_col[0]\n",
    "    val_dfl_col = val_dfl_col[0]\n",
    "    \n",
    "    # Find the epoch with the best validation loss\n",
    "    best_epoch = results_df[val_dfl_col].idxmin()\n",
    "    best_val_loss = results_df.loc[best_epoch, val_dfl_col]\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot training and validation losses\n",
    "    plt.plot(results_df['epoch'], results_df[train_dfl_col], label='Train DFL Loss')\n",
    "    plt.plot(results_df['epoch'], results_df[val_dfl_col], label='Validation DFL Loss')\n",
    "    \n",
    "    # Mark the best model with a vertical line\n",
    "    plt.axvline(x=results_df.loc[best_epoch, 'epoch'], color='r', linestyle='--', \n",
    "                label=f'Best Model (Epoch {int(results_df.loc[best_epoch, \"epoch\"])}, Val Loss: {best_val_loss:.4f})')\n",
    "    \n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('DFL Loss')\n",
    "    plt.title('Training and Validation DFL Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Save the plot in the same directory as weights\n",
    "    plot_path = os.path.join(run_dir, 'dfl_loss_curve.png')\n",
    "    plt.savefig(plot_path)\n",
    "    \n",
    "    # Also save it to the working directory for easier access\n",
    "    plt.savefig(os.path.join('kaggle/working', 'dfl_loss_curve.png'))\n",
    "    \n",
    "    print(f\"Loss curve saved to {plot_path}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Return the best epoch info\n",
    "    return best_epoch, best_val_loss\n",
    "\n",
    "\n",
    "\n",
    "def train_yolo_model(yaml_path, pretrained_weights_path, epochs=30, batch_size=12, img_size=640):\n",
    "    \"\"\"\n",
    "    Train a YOLO model on the prepared dataset\n",
    "    \n",
    "    Args:\n",
    "        yaml_path (str): Path to the dataset YAML file\n",
    "        pretrained_weights_path (str): Path to pre-downloaded weights file\n",
    "        epochs (int): Number of training epochs\n",
    "        batch_size (int): Batch size for training\n",
    "        img_size (int): Image size for training\n",
    "    \"\"\"\n",
    "    print(f\"Loading pre-trained weights from: {pretrained_weights_path}\")\n",
    "    \n",
    "    # Load a pre-trained YOLOv8 model\n",
    "    model = YOLO(pretrained_weights_path)\n",
    "    \n",
    "    # Train the model with early stopping\n",
    "    results = model.train(\n",
    "        data=yaml_path,\n",
    "        epochs=epochs,\n",
    "        batch=batch_size,\n",
    "        imgsz=img_size,\n",
    "        project=\"yolo_weights_dir\",\n",
    "        name='motor_detector',\n",
    "        exist_ok=True,\n",
    "        patience=20,              # Early stopping if no improvement for 5 epochs\n",
    "        save_period=5,           # Save checkpoints every 5 epochs\n",
    "        val=True,                # Ensure validation is performed\n",
    "        verbose=True\n",
    "                                  # Show detailed output during training\n",
    "    )\n",
    "    \n",
    "    # Get the path to the run directory\n",
    "    run_dir = os.path.join(yolo_weights_dir, 'motor_detector')\n",
    "    \n",
    "    # Plot and save the loss curve\n",
    "    best_epoch_info = plot_dfl_loss_curve(run_dir)\n",
    "    \n",
    "    if best_epoch_info:\n",
    "        best_epoch, best_val_loss = best_epoch_info\n",
    "        print(f\"\\nBest model found at epoch {best_epoch} with validation DFL loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return model, results\n",
    "\n",
    "\n",
    "\n",
    "def predict_on_samples(model, num_samples=4):\n",
    "    \"\"\"\n",
    "    Run predictions on random validation samples and display results\n",
    "    \n",
    "    Args:\n",
    "        model: Trained YOLO model\n",
    "        num_samples (int): Number of random samples to test\n",
    "    \"\"\"\n",
    "    # Get validation images\n",
    "    val_dir = os.path.join(yolo_dataset_dir, 'images', 'val')\n",
    "    if not os.path.exists(val_dir):\n",
    "        print(f\"Validation directory not found at {val_dir}\")\n",
    "        # Try train directory instead if val doesn't exist\n",
    "        val_dir = os.path.join(yolo_dataset_dir, 'images', 'train')\n",
    "        print(f\"Using train directory for predictions instead: {val_dir}\")\n",
    "        \n",
    "    if not os.path.exists(val_dir):\n",
    "        print(\"No images directory found for predictions\")\n",
    "        return\n",
    "    \n",
    "    val_images = os.listdir(val_dir)\n",
    "    \n",
    "    if len(val_images) == 0:\n",
    "        print(\"No images found for prediction\")\n",
    "        return\n",
    "    \n",
    "    # Select random samples\n",
    "    num_samples = min(num_samples, len(val_images))\n",
    "    samples = random.sample(val_images, num_samples)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, img_file in enumerate(samples):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "            \n",
    "        img_path = os.path.join(val_dir, img_file)\n",
    "        \n",
    "        # Run prediction\n",
    "        results = model.predict(img_path, conf=0.25)[0]\n",
    "        \n",
    "        # Load and display the image\n",
    "        img = Image.open(img_path)\n",
    "        axes[i].imshow(np.array(img), cmap='gray')\n",
    "        \n",
    "        # Draw ground truth box if available (from filename)\n",
    "        try:\n",
    "            # This assumes your filenames contain coordinates in a specific format\n",
    "            parts = img_file.split('_')\n",
    "            y_part = [p for p in parts if p.startswith('y')]\n",
    "            x_part = [p for p in parts if p.startswith('x')]\n",
    "            \n",
    "            if y_part and x_part:\n",
    "                y_gt = int(y_part[0][1:])\n",
    "                x_gt = int(x_part[0][1:].split('.')[0])\n",
    "                \n",
    "                box_size = 24\n",
    "                rect_gt = Rectangle((x_gt - box_size//2, y_gt - box_size//2), \n",
    "                              box_size, box_size, \n",
    "                              linewidth=1, edgecolor='g', facecolor='none')\n",
    "                axes[i].add_patch(rect_gt)\n",
    "        except:\n",
    "            pass  # Skip ground truth if parsing fails\n",
    "        \n",
    "        # Draw predicted boxes (red)\n",
    "        if len(results.boxes) > 0:\n",
    "            boxes = results.boxes.xyxy.cpu().numpy()\n",
    "            confs = results.boxes.conf.cpu().numpy()\n",
    "            \n",
    "            for box, conf in zip(boxes, confs):\n",
    "                x1, y1, x2, y2 = box\n",
    "                rect_pred = Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                     linewidth=1, edgecolor='r', facecolor='none')\n",
    "                axes[i].add_patch(rect_pred)\n",
    "                axes[i].text(x1, y1-5, f'{conf:.2f}', color='red')\n",
    "        \n",
    "        axes[i].set_title(f\"Image: {img_file}\\nGround Truth (green) vs Prediction (red)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the predictions plot\n",
    "    plt.savefig(os.path.join('kaggle/working', 'predictions.png'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Check and create a dataset YAML if needed\n",
    "def prepare_dataset():\n",
    "    \"\"\"\n",
    "    Check if dataset exists and create a proper YAML if needed\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the YAML file to use for training\n",
    "    \"\"\"\n",
    "    # Check if images exist\n",
    "    train_images_dir = os.path.join(yolo_dataset_dir, 'images', 'train_augmented')\n",
    "    val_images_dir = os.path.join(yolo_dataset_dir, 'images', 'val')\n",
    "    train_labels_dir = os.path.join(yolo_dataset_dir, 'labels', 'train_augmented')\n",
    "    val_labels_dir = os.path.join(yolo_dataset_dir, 'labels', 'val')\n",
    "    \n",
    "    # Print directory existence status\n",
    "    print(f\"Directory status:\")\n",
    "    print(f\"- Train images dir exists: {os.path.exists(train_images_dir)}\")\n",
    "    print(f\"- Val images dir exists: {os.path.exists(val_images_dir)}\")\n",
    "    print(f\"- Train labels dir exists: {os.path.exists(train_labels_dir)}\")\n",
    "    print(f\"- Val labels dir exists: {os.path.exists(val_labels_dir)}\")\n",
    "    \n",
    "    # Check for original YAML file\n",
    "    original_yaml_path = os.path.join(yolo_dataset_dir, 'dataset.yaml')\n",
    "    \n",
    "    if os.path.exists(original_yaml_path):\n",
    "        print(f\"Found original dataset.yaml at {original_yaml_path}\")\n",
    "        # Fix the paths in the YAML\n",
    "        return fix_yaml_paths(original_yaml_path)\n",
    "    else:\n",
    "        print(f\"Original dataset.yaml not found, creating a new one\")\n",
    "        \n",
    "        # Create a new YAML file\n",
    "        yaml_data = {\n",
    "            'path': yolo_dataset_dir,\n",
    "            'train': 'images/train_augmented',\n",
    "            'val': 'images/train_augmented' if not os.path.exists(val_images_dir) else 'images/val',\n",
    "            'names': {0: 'motor'}\n",
    "        }\n",
    "        \n",
    "        new_yaml_path = \"kaggle/working/dataset.yaml\"\n",
    "        with open(new_yaml_path, 'w') as f:\n",
    "            yaml.dump(yaml_data, f)\n",
    "            \n",
    "        print(f\"Created new YAML at {new_yaml_path}\")\n",
    "        return new_yaml_path\n",
    "\n",
    "\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    print(\"Starting YOLO training process...\")\n",
    "    \n",
    "    # Prepare dataset and get YAML path\n",
    "    yaml_path = prepare_dataset()\n",
    "    print(f\"Using YAML file: {yaml_path}\")\n",
    "    \n",
    "    # Print YAML file contents\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        yaml_content = f.read()\n",
    "    print(f\"YAML file contents:\\n{yaml_content}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nStarting YOLO training...\")\n",
    "    model, results = train_yolo_model(\n",
    "        yaml_path,\n",
    "        pretrained_weights_path=yolo_pretrained_weights,\n",
    "        epochs=30  # Using 30 epochs instead of 100 for faster training\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining complete!\")\n",
    "    \n",
    "    # Run predictions\n",
    "    print(\"\\nRunning predictions on sample images...\")\n",
    "    predict_on_samples(model, num_samples=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flag_motor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
